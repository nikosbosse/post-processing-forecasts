---
title: "Post-Processing COVID-19 Forecasts"
subtitle: "- Forecasting Hub Presentation -<br><br>"
author: "Matthias Herp & Joel Beck"
date: "11.02.2022"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current% / %total%"
      beforeInit: "https://platform.twitter.com/widgets.js"
      # List of highlighting styles: https://highlightjs.org/static/demo/
      # not all of them work with xaringan
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      ratio: 16:9
      countIncrementalSlides: false
    # Adds logo to all slides except title slide
    # set slide class 'hide_logo' to hide logo on particular slides
    # modify insert-logo.html file with correct file path to logo and desired 
    # logo position 
    # https://www.garrickadenbuie.com/blog/xaringan-tip-logo-all-slides/
    # includes:
    #   after_body: insert-logo.html
---
<!-- here classes for second slide -->

```{r, child="xaringan-setup.Rmd", echo=FALSE}

```

```{r, include=FALSE}
devtools::load_all(".")
library(dplyr)
library(patchwork)
library(ggplot2)

# cache chunks that take a long time
cache <- FALSE

uk_data <- read.csv(here::here("data", "full-data-uk-challenge.csv"))
#df_subset <- slice_sample(df, n = 50)

uk_complete <- readr::read_rds(here::here("data_results", "uk_complete.rds"))
hub_1 <- readr::read_rds(here::here("data_results", "hub_cqr2_1.rds"))
hub_2 <- readr::read_rds(here::here("data_results", "hub_cqr2_2.rds"))
hub_cqr <- dplyr::bind_rows(hub_1, hub_2)

# Ensemble in uk_complete
#uk_cqr_qsa_ensemble <- readr::read_rds(
#  here::here("data_results", "uk_cqr_qsa_uniform_ensemble.rds")
#)
hub_cqr_qsa_ensemble <- readr::read_rds(
  here::here("data_results", "hub_cqr_qsa_uniform_ensemble_subset.rds")
)

# helper functions to round values in data frame for nicer display in slides
round_output <- function(df, digits) {
  df |> mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, digits)))
}

display_table <- function(df, digits = 3, align = "left") {
  df |>
    round_output(digits = digits) |>
    gt::gt() |>
    gt::tab_options(
      table.align = align, row.striping.include_table_body = TRUE,
      data_row.padding = gt::px(15)
    )
}
```

<!-- here content of second slide -->

## Motivation:<br>UK Covid-19 Crowd Forecasting Challenge

.footnote[ 
https://www.crowdforecastr.org/2021/05/11/uk-challenge/ <br>
https://epiforecasts.io/uk-challenge/
]

- Idea: Compare forecasts from humans with model-based predictions of Covid-19 Cases and Deaths for the next 4 weeks in the United Kingdom

--

- Empirically human forecasts are surprisingly competitive and in some cases even better than statistical models 

--

- This is mostly true for **point** forecasts, prediction **intervals** are often chosen too narrow, i.e. humans tend to be too confident in their own predictions

--

- Goal: Use valuable information from point forecasts and adjust prediction intervals / quantile forecasts with an appropriate correction procedure  

--

- Part of ongoing research project by the **epiforecasts** group at the London School of Hygiene & Tropical Medicine where our project supervisor Nikos Bosse is engaged as a doctoral candidate

---

## Setting

- Post-Processing Covid19 forecasts: Systematically adjust existing prediction intervals with the goal of better out-of-sample performance

--

- Original forecasts from two data sources: The **UK Covid-19 Crowd Forecasting Challenge**<sup>1</sup> (includes forecasts of non-expert individuals) and the **European Forecast Hub**<sup>2</sup> (forecasts from international research groups)

.footnote[ 
[1] https://www.crowdforecastr.org/2021/05/11/uk-challenge/ <br>
[2] https://covid19forecasthub.eu/index.html
]

--

- We consider five dimensions: **location**, **model**, **target type**, **horizon** and **quantile**

--

- The quality of prediction intervals is measured by the **Weighted Interval Score** based on a trade-off between interval coverage and precision  

---

## Evaluation

- Based on **Weighted Interval Score**<sup>1</sup>  

.center.bold[WIS = Sharpness + Overprediction + Underprediction]

.footnote[[1] https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618]

--

  - For a given quantile level $\alpha$, true observed value $y$ as well as lower bound $l$ and upper bound $u$ of the corresponding $(1 - \alpha) \cdot 100$% prediction interval, the score is computed as

$$
Score_\alpha(y) = (u-l) + \frac{2}{\alpha} \cdot (l-y) \cdot \mathbf{1} (y \leq l) + \frac{2}{\alpha} \cdot (y-u) \cdot \mathbf{1}(y \geq u)
$$

--

- The Score of the entire model can be obtained from a weighted sum over all (included) quantile levels $\alpha$  

--

- Implemented in the .mono[**scoringutils**] R package written by Nikos

---
class: inverse, center, middle

# The .mono[postforecasts] package `r emo::ji("package")`

---

## Core Idea

- Structured and unifying framework for implementing various post-processing techniques

--

- Aims to establish a consistent workflow for a collection of post-processing methods
<!-- supported by an intuitive user interface  -->

--

- Allows for convenient comparisons between methods for the data of interest
<!-- and the choice of the best method -->

---

## Code example

.left-30[

- Sub-samples of the data can be specified along all five data dimensions

- **cv_init_training** sets the end point of the training set

- Results can be extracted for the validation set separately

]

.right-65[

```{r, results="hide"}
df_updated <- update_predictions(
    df = uk_data, 
    methods = "cqr", 
    models = "epiforecasts-EpiExpert",
    cv_init_training = 5, 
    horizons = 4, 
    verbose = FALSE, 
    parallel = FALSE
  )
df_combined <- df_updated |> collect_predictions()
```

```{r}
df_combined |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = c("method")) |>
  dplyr::select(-c("coverage_deviation","bias","aem")) |>
  display_table()
```

]

---
class: inverse, middle, center

# Conformalized Quantile Regression

---

## CQR Algorithm

Theory based on Paper Romano Y., Patterson E., and Candès E. (2019): *Conformalized Quantile Regression*

**Step 1:** <br>
Split the data into a training and validation (here called *calibration*) set, indexed by $I_1$ and $I_2$, respectively.

--

**Step 2:** <br>
For a given quantile $\alpha$ and a given quantile regression algorithm $\mathcal{A}$, calculate lower and upper interval bounds on the training set:

$$
\begin{aligned}
\left\{ \hat{ q}_{\alpha, low}, \; \hat{ q}_{\alpha, high} \right\} \leftarrow \mathcal{A} \left( \left\{ (X_i, Y_i): i \in I_1 \right\} \right) 
\end{aligned}
$$

--

**Step 3:** <br>
Compute **conformity scores** on the calibration set:

$$
\begin{aligned}
E_i := \operatorname*{max} \left\{ \hat{ q}_{\alpha, low}(X_i) - Y_i, \; Y_i - \hat{ q}_{\alpha, high}(X_i) \right\} \quad \forall \; i \in I_2
\end{aligned}
$$

For each $i$, the corresponding score $E_i$ is **positive** if $Y_i$ is **outside** the interval $\left[ \hat{ q}_{\alpha, low}(X_i), \; \hat{ q}_{\alpha, high}(X_i) \right]$ and **negative** if $Y_i$ is **inside** the interval.

---

## CQR Algorithm

**Step 4:** <br>
Compute the **margin** $Q_{1 - \alpha}(E, I_2)$ given by the $(1 - \alpha)(1 + \frac{ 1}{ 1 + \left| I_2 \right| })$-th empirical quantile of the scores $E_i$ in the calibration set.

--

**Step 5:** <br>
On the basis of the original prediction interval bounds $\hat{ q}_{\alpha, low}(X_i)$ and $\hat{ q}_{\alpha, high}(X_i)$, the new *post-processed* prediction interval for $Y_i$ is given by:

$$
\begin{aligned}
C(X_{n+1}) = \left[ \hat{ q}_{\alpha,  low}(X_i) - Q_{1 - \alpha}(E, I_2), \; \hat{ q}_{\alpha,  high}(X_i) + Q_{1 - \alpha}(E, I_2) \right].
\end{aligned}
$$
---

## CQR Extensions

We propose a **asymmetric** extension of CQR that uses different margins for the upper and lower bounds

--

**Step 3:** <br>
Compute **upper and lower bound conformity scores** on the calibration set:

$$
\begin{aligned}
E_{i, low} &:= \hat{ q}_{\alpha, low}(X_i) - Y_i \quad \forall \; i \in I_2 \\
E_{i, high} &:= Y_i - \hat{ q}_{\alpha, high}(X_i) \quad \forall \; i \in I_2 
\end{aligned}
$$
--

**Step 5:** <br>
The new *post-processed* prediction interval for $Y_i$ is given by:

$$
\begin{aligned}
C(X_{n+1}) = \left[ \hat{ q}_{\alpha, low}(X_i) - Q_{1 - \alpha, low}(E_{low}, I_2), \; \hat{ q}_{\alpha, high}(X_i) + Q_{1 - \alpha, high}(E_{high}, I_2) \right].
\end{aligned}
$$
---

## CQR Extensions

We also propose a **asymmetric and multiplicative** extension of CQR that uses relative margins bounds

--

**Step 3:** <br>
Compute **upper and lower bound conformity scores** on the calibration set:

$$
\begin{aligned}
E_{i, low} &:= \frac{ Y_i}{ \hat{ q}_{\alpha, low}(X_i)} \quad \forall \; i \in I_2 \\
E_{i, high} &:= \frac{ Y_i}{ \hat{ q}_{\alpha, high}(X_i)} \quad \forall \; i \in I_2,
\end{aligned}
$$

--

**Step 5:** <br>
The new *post-processed* prediction interval for $Y_i$ is given by:

$$
\begin{aligned}
C(X_{n+1}) = \left[ \hat{ q}_{\alpha, low}(X_i) \cdot Q_{1 - \alpha, low}(E_{low}, I_2), \; \hat{ q}_{\alpha, high}(X_i) \cdot Q_{1 - \alpha, high}(E_{high}, I_2) \right].
\end{aligned}
$$

---

## CQR Extensions

While the idea of multiplicative correction terms is appealing, it turns out that the approach above is flawed in two ways:

--

1. It is very outlier sensitive and tends to **produce huge margins** for short time series.

--

--> A possible solution is to **regularize the conformity scores** by a root transformation:

$$
\begin{aligned}
E_{i, low}^{reg} = E_{i, low}^{ \left( \frac{ 1}{ \sigma_{E_{low}}} \right)}, \quad 
E_{i, high}^{reg} = E_{i, high}^{ \left( \frac{ 1}{ \sigma_{E_{high}}} \right)},
\end{aligned}
$$
--

2. It can result in **extreme intervals shifts**, in our case typically upward.

--

--> To counteract this we restrained the shifts to both either decrease or increase the interval:

$$
\begin{aligned}
Q_{1 - \alpha, low} \cdot Q_{1 - \alpha, high} \stackrel{ !}{ =} 1.
\end{aligned}
$$

---

## CQR Results

```{r, ch2-uk-cqr3-intervals, echo=FALSE, fig.cap="Comparison of CQR variations on the UK data set.", out.width="80%"}
uk_cqr3 <- readr::read_rds(here::here("data_results", "uk_cqr3.rds"))

plot_intervals(
  df = uk_cqr3, model = "epiforecasts-EpiExpert_direct", target_type = "Cases",
  quantile = 0.2, horizon = 2
)
```

---
class: inverse, center, middle

# Quantile Spread Averaging (QSA)

---

## QSA - Intuition

.left-30[ 
- Three possibilities to define quantile spreads

- We choose median based quantile spreads due to the advantages:
    + spreads are independent of quantile number 
    + non-symmetric adjustments are possible

- Disadvantage: quantile crossing

]

.right-65[ 

![](Images/qs_all.png)

]

---

## QSA - Theory 

Let $n$ specify the number of observations in the training set within this combination, $\mathbf{y} \in \mathbb{R}^n$ the vector of true values and $\hat{\mathbf{q}}_1, \ldots, \hat{\mathbf{q}}_p \in \mathbb{R}^n$ vectors of quantile estimates for $p$ different probability levels.

Then, for each time series, the quantile spread adjustment computes the quantile spread factors $\mathbf{w}^* \in \mathbb{R}^p$ by minimizing the weighted interval score:
$$
\begin{aligned}
\mathbf{w}^*
&= \operatorname*{arg\,min}_{\mathbf{w} \in \mathbb{R}^p} WIS_\alpha(\mathbf{y}) \\
&= \operatorname*{arg\,min}_{\mathbf{w} \in \mathbb{R}^p} \sum_{i=1}^p \sum_{j=1}^n (u_{i,j}^*-l_{i,j}^*) + \frac{2}{\alpha} \cdot (l_{i,j}^*-y_j) \cdot \mathbf{1} (y_j \leq l_{i,j}^*) + \frac{2}{\alpha} \cdot (y_j-u_{i,j}^*) \cdot \mathbf{1}(y_j \geq u_{i,j}^*) \\
\text{s.t.} \qquad l_{i,j}^* &= l_{i,j} + (l_{i,j}-m) \cdot w_i \quad \text{and} \quad 
u_{i,j}^* = u_{i,j} + (u_{i,j}-m) \cdot w_i 
\end{aligned}
$$

--

The optimization uses the **optim** function from the **stats**<sup>1</sup> package. 
As optimization method we use the quasi-Newton method **BFGS** named after Broyden, Fletcher, Goldfarb and Shanno.

<!-- read in preparation: https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm and find reference to the paper for footnotes. -->

.footnote[ 
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optim
]

---

## QSA - Flavors and Extensions 

The **postforecasts** package offers three flavors of QSA where each restricts $\mathbf{w}$ differently:
- uniform: $i \in [0, 1, \ldots, p-1, p] \quad w_i = c$
- flexibel: No Restrictions
- flexibel-symmetric: $i \in [0, 1, \ldots, m-1] \quad w_i = w_{p-i}$

--

Furthermore, **postforecasts** provides regularization towards QSA Uniform by adding a regularization term $Pen(w)$ with the weight $r$ to the score function: 

$$
\begin{aligned}
\mathbf{w}^*
&= \operatorname*{arg\,min}_{\mathbf{w} \in \mathbb{R}^p} \ WIS_\alpha(\mathbf{y}) + r \cdot Pen(\mathbf{w}), \quad Pen(\mathbf{w}) = \sum_{i=1}^p (w_i - \bar{w})^2 \\
\text{s.t.} \qquad \bar{w} &= \frac{1}{p} \sum_{i=1}^p w_i
\end{aligned}
$$
---
class: inverse, center, middle

# Ensemble Model

---

---

## Ensemble Model

Predictions for a combination of location, model, target type, horizon and quantile are computed as a convex linear combination of the individual methods. 

$$
\begin{aligned}
\mathbf{w}^*
= \operatorname*{arg\,min}_{ \mathbf{w} \in [0, 1]^k} IS_\alpha(\mathbf{y})
&= \operatorname*{arg\,min}_{ \mathbf{w} \in [0, 1]^k} (\mathbf{u}-\mathbf{l}) + \frac{2}{\alpha} \cdot (\mathbf{l}-\mathbf{y}) \cdot 1(\mathbf{y} \leq \mathbf{l}) + \frac{2}{\alpha} \cdot (\mathbf{y}-\mathbf{u}) \cdot 1(\mathbf{y} \geq \mathbf{u}), \\
\text{with} \qquad \mathbf{l} &= \sum_{j=1}^{k} w_j \mathbf{l}_j, \;\; \mathbf{u} = \sum_{j=1}^{k} w_j \mathbf{u}_j \\
\text{s.t.} \qquad \left \Vert \mathbf{w} \right \Vert_1 &= \sum_{j=1}^{k} w_j = 1,
\end{aligned}
$$
--

Advantages of this method are:

1. Predictions remain in the same scale as the individual methods

2. Interpretable coefficients as fractional contribution of experts

---
class: inverse, center, middle

# Results

---

## Comparison of Prediction Intervals for all Post-Processing Methods including the Ensemble

```{r, include=FALSE}
weights_df <- attr(uk_complete, which = "weights")

weights_only <- do.call("rbind", weights_df$weights) |>
  magrittr::set_colnames(value = c(
    "cqr",  "cqr_asymmetric", "qsa_uniform", "qsa_flexible_symmetric",
    "qsa_flexible"
  )) |>
  as_tibble()

weights_full <- bind_cols(
  weights_df |> select(-c(location, weights)),
  weights_only
) |>
  filter(quantile < 0.5) |>
  rowwise() |>
  mutate(rowsum = sum(c_across(cols = cqr:qsa_flexible))) |>
  mutate(max_weight = max(c_across(cols = cqr:qsa_flexible))) |>
  ungroup()
```

```{r, ch4-intervals, echo=FALSE}
mod <- "seabbs"
t <- "Cases"
h <- 4
q <- 0.025

plot_weights <- weights_full |>
  filter(model == mod, target_type == t, horizon == h, quantile == q) |>
  mutate(across(.cols = cqr:qsa_flexible, .fns = ~ round(.x, digits = 4)))

plot_df <- uk_complete |>
  mutate(method = case_when(
    method == "cqr" ~ stringr::str_glue(
      "cqr\nweight: {plot_weights$cqr}"
    ),
    method == "cqr_asymmetric" ~ stringr::str_glue(
      "cqr_asymmetric\nweight: {plot_weights$cqr_asymmetric}"
    ),
    method == "qsa_uniform" ~ stringr::str_glue(
      "qsa_uniform\nweight: {plot_weights$qsa_uniform}"
    ),
    method == "qsa_flexible_symmetric" ~ stringr::str_glue(
      "qsa_flexible_symmetric\nweight: {plot_weights$qsa_flexible_symmetric}"
    ),
    method == "qsa_flexible" ~ stringr::str_glue(
      "qsa_flexible\nweight: {plot_weights$qsa_flexible}"
    ),
    TRUE ~ method
  ))

plot_intervals(
  plot_df,
  model = mod, target_type = t, quantile = q, horizon = h, base_size = 8
) +
  scale_color_manual(
    values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#e6ab02", "#a65628")
  )
```

---

# Overall Method Comparison Results

.left-30[

- Results only for the UK data due to computational demands of QSA

- Ensemble is clearly the best model in the aggregate

- QSA methods tended to be better than CQR

]

.right-65[

```{r, ch4-wis-comparison, echo=FALSE}
tab_training <- uk_complete |>
  extract_training_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = "method") |>
  select(method:dispersion) |>
  rename(`training score` = interval_score)

tab_validation <- uk_complete |>
  extract_validation_set() |>
  scoringutils::score() |>
  scoringutils::summarise_scores(by = "method") |>
  select(method:dispersion) |>
  rename(`validation score` = interval_score)

tab_validation |>
  mutate(`training score` = tab_training$`training score`) |>
  relocate(`training score`, .after = `validation score`) |>
  arrange(`validation score`) |>
  display_table()
```

]

---

## Method Comparison for each Forecasting Model and Target Type.

```{r, ch4-model-target, echo=FALSE}
df_method_model <- eval_methods(uk_complete, summarise_by = "model")
p1 <- plot_eval(df_method_model, base_size = 7) +
  labs(y = NULL, title = "Method Comparison by Model and Target Type", subtitle = NULL) +
  theme(axis.text.x = element_blank())

df_method_target_type <- eval_methods(uk_complete, summarise_by = "target_type")
p2 <- plot_eval(df_method_target_type, base_size = 7) +
  labs(y = NULL, title = NULL, subtitle = NULL)

p1 / p2
```

---

## Method Comparison for each Forecast Horizon and Quantile Level

```{r, ch4-horizon-quantile, echo=FALSE}
df_method_horizon <- eval_methods(uk_complete, summarise_by = "horizon")
p1 <- plot_eval(df_method_horizon, base_size = 7) +
  labs(title = "Method Comparison by Horizon and Quantile", subtitle = NULL) +
  theme(axis.text.x = element_blank())

b <- "black"
t <- "transparent"

df_method_quantile <- eval_methods(uk_complete, summarise_by = "quantile")
p2 <- plot_eval(df_method_quantile, base_size = 7) +
  labs(title = NULL, subtitle = NULL) +
  theme(
    axis.text.y = element_text(color = c(b, rep(c(t, b), 11)))
  )

p1 / p2
```

---

## CQR on the Hub Data

```{r, ch2-hub-cqr-eval, echo=FALSE}
hub_cqr_no_poland <- hub_cqr |> filter(location_name != "Poland")

b <- "black"
t <- "transparent"

df_cqr_hub_location_quantile <- eval_methods(
  hub_cqr_no_poland,
  summarise_by = c("location_name", "quantile")
)
p1 <- plot_eval(df_cqr_hub_location_quantile, base_size = 7) +
  labs(
    y = NULL,
    title = "CQR Performance by\nCountry and Quantile",
    subtitle = NULL
  ) +
  theme(
    axis.text.x = element_text(color = c(b, rep(c(t, t, b, t), 5), t, b))
  )

df_cqr_hub_location_horizon <- eval_methods(
  hub_cqr_no_poland,
  summarise_by = c("location_name", "horizon")
)
p2 <- plot_eval(df_cqr_hub_location_horizon, base_size = 7) +
  labs(
    y = NULL,
    title = "CQR Performance by \nCountry and Horizon",
    subtitle = NULL
  )

p1 + p2
```

---

## CQR Anomalie Poland

```{r, ch2-hub-cqr-location, echo=FALSE, out.width="70%"}
df_cqr_hub_location <- eval_methods(hub_cqr, summarise_by = c("location_name"))
plot_eval(df_cqr_hub_location, heatmap = FALSE) + labs(y = NULL, title = "CQR Improvements of Weighted Interval Score by Country")
```

---

## CQR Anomalie Poland

```{r, ch2-hub-cqr-poland-intervals, echo=FALSE}
cqr_poland <- hub_cqr |> filter(location_name == "Poland")

p1 <- cqr_poland |>
  plot_intervals(model = "IEM_Health-CovidProject", target_type = "Cases", base_size = 7) +
  labs(title = "Predicted Cases for Poland 1 week ahead")

p2 <- hub_cqr |>
  filter(location == "DE") |>
  plot_intervals(model = "IEM_Health-CovidProject", target_type = "Cases", base_size = 7) +
  labs(title = "Predicted Cases for Germany 1 week ahead")

p1 + p2
```

--

---
class: inverse, center, middle

# Conclusion

---

## Conclusion

- We presented CQR and QSA model flavors for post processing

- For the UK data, post processing improved human forecasts, in particular QSA improved forecasts to a greater amount than CQR

- Furthermore we found an Ensemble of methods to provide thee best adjustments out of sample

- In the Hub data we showed that with CQR post-processing can even benefit model predictions

---

## Possible Next Steps

- Examine QSA and Ensemble adjustments for the Hub data

- Examine Regularized QSA results for both data sets

- Use exponential smoothing to weight observations depending on how far back they where for CQR and QSA

- Apply the methods to further time series analysis domains
